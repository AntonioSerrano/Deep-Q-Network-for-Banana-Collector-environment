{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feedforward neural network with two hidden layers plus the final layer. The hidden layers are made of 64 units each. The final layer outputs a predicted Q-value for each of the possible 4 actions. The non-linear activation function selected is ReLU. No dropout layers were added to prevent overfitting.\n",
    "\n",
    "In addition, a dueling architecture is implemented and activated by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "\n",
    "        self.fc2_adv = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc2_val = nn.Linear(fc1_units, fc2_units)\n",
    "\n",
    "        self.fc3_adv = nn.Linear(fc2_units, action_size)\n",
    "        self.fc3_val = nn.Linear(fc2_units, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        y = F.relu(self.fc1(state))\n",
    "\n",
    "        adv = F.relu(self.fc2_adv(y))\n",
    "        val = F.relu(self.fc2_val(y))\n",
    "\n",
    "        adv = self.fc3_adv(adv)\n",
    "        val = self.fc3_val(val)\n",
    "\n",
    "        x = val + adv - torch.mean(adv, dim=1, keepdim=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "ALPHA = 0.6             # degree of randomness for sampling probabilty (0 = pure uniform randomess; 1 = only use priorities)\n",
    "BETA = 0.4              # initial value for beta, which controls how much importance weights affect learning\n",
    "BETA_ITERS = 25000      # number of iterations over which beta will be annealed from initial value to 1\n",
    "EPSILON_PER = 0.2       # prioritized experience replay epsilon. A very small number just to prevent zero divion for probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Double Q-learning and Prioritized Experience Replay are implemented and activated by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the GPU by default, if available:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed, ALPHA)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "        # Initialize learning step for updating beta\n",
    "        self.learn_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get prioritized subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA, BETA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma, beta):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "            beta (float): initial value for beta, which controls how much importance weights affect learning\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones, probabilities, indices = experiences\n",
    "\n",
    "        ### Double Q-learning:\n",
    "        # Get the Q values for each next_state, action pair from the \n",
    "        # local/online/behavior Q network:\n",
    "        Q_targets_next_local = self.qnetwork_local(next_states).detach()\n",
    "        # Get the corresponding best action for those next_states:\n",
    "        _, a_prime = Q_targets_next_local.max(1)\n",
    "\n",
    "        # Get the Q values from the target Q network but following a_prime,\n",
    "        # which belongs to the local network, not the target network:\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach()\n",
    "        Q_targets_next = Q_targets_next.gather(1, a_prime.unsqueeze(1))\n",
    "        ###\n",
    "\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)   \n",
    "\n",
    "        # Compute and update new priorities\n",
    "        new_priorities = (abs(Q_expected - Q_targets) + EPSILON_PER).detach()\n",
    "        self.memory.update_priority(new_priorities, indices)\n",
    "\n",
    "        # Update beta parameter (b). By default beta will reach 1 after \n",
    "        # 25,000 training steps (~325 episodes in the Banana environment):\n",
    "        b = min(1.0, beta + self.learn_step * (1.0 - beta) / BETA_ITERS)\n",
    "        self.learn_step += 1\n",
    "\n",
    "        # Compute and apply importance sampling weights to TD Errors\n",
    "        ISweights = (((1 / len(self.memory)) * (1 / probabilities)) ** b)\n",
    "        max_ISweight = torch.max(ISweights)\n",
    "        ISweights /= max_ISweight\n",
    "        Q_targets *= ISweights\n",
    "        Q_expected *= ISweights\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952) implementation  is not mine. It was borrowed from [austinsilveria](https://github.com/austinsilveria/Banana-Collection-DQN), which is based on an unsorted [sum tree model](https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/) to sample the tuples from the replay buffer more efficiently based on the TD error criteria. Consequently, a separate class to handle sum tree model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, alpha):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = SumTree(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"priority\"])\n",
    "        # Set alpha parameter to control the degree of uniform random \n",
    "        # sampling:\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 0\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, priority=10):\n",
    "        \"\"\"Add a new experience to memory including its priority to be later sampled from the experience pool\"\"\"\n",
    "\n",
    "        # If the experience pool is smaller than the batch size, then assign\n",
    "        # the maximum priority bu default:\n",
    "        if len(self.memory) > self.batch_size + 5:\n",
    "            e = self.experience(state, action, reward, next_state, done, self.max_priority)\n",
    "        # Otherwise assign priority to the power of alpha:\n",
    "        else:\n",
    "            e = self.experience(state, action, reward, next_state, done, int(priority) ** self.alpha)\n",
    "        self.memory.add(e)\n",
    "\n",
    "    def update_priority(self, new_priorities, indices):\n",
    "        \"\"\"Updates priority of experience after learning.\"\"\"\n",
    "        for new_priority, index in zip(new_priorities, indices):\n",
    "            old_e = self.memory[index]\n",
    "            new_p = new_priority.item() ** self.alpha\n",
    "            new_e = self.experience(old_e.state, old_e.action, old_e.reward, old_e.next_state, old_e.done, new_p)\n",
    "            #self.memory.update(index, new_e)\n",
    "            self.memory.update(index.item(), new_e)\n",
    "            if new_p > self.max_priority:\n",
    "                self.max_priority = new_p\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the memory pool based on TD Error\n",
    "        priority. Return indices of sampled experiences in order to update \n",
    "        their priorities after learning from them.\n",
    "        \"\"\"\n",
    "        experiences = []\n",
    "        indices = []\n",
    "        sub_array_size = self.memory.get_sum() / self.batch_size\n",
    "        for i in range(self.batch_size):\n",
    "            choice = np.random.uniform(sub_array_size * i, sub_array_size * (i + 1))\n",
    "            e, index = self.memory.retrieve(1, choice)\n",
    "            experiences.append(e)\n",
    "            indices.append(index)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        probabilities = torch.from_numpy(np.vstack([e.priority / self.memory.get_sum() for e in experiences])).float().to(device)\n",
    "        indices = torch.from_numpy(np.vstack([i for i in indices])).int().to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, probabilities, indices\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "class SumTree:\n",
    "    \"\"\"\n",
    "    Leaf nodes hold experiences and intermediate nodes store experience priority sums.\n",
    "    Adapted from: https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxlen):\n",
    "        \"\"\"Initialize a SumTree object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            maxlen (int): maximum size of replay buffer\n",
    "        \"\"\"\n",
    "        self.sumList = np.zeros(maxlen*2)\n",
    "        self.experiences = np.zeros(maxlen*2, dtype=object)\n",
    "        self.maxlen = maxlen\n",
    "        self.currentSize = 0\n",
    "        # Set insertion marker for next item as first leaf\n",
    "        self.tail = ((len(self.sumList)-1) // 2) + 1\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Add experience to array and experience priority to sumList.\"\"\"\n",
    "        if self.tail == len(self.sumList):\n",
    "            self.tail = ((len(self.sumList)-1) // 2) + 1\n",
    "        self.experiences[self.tail] = experience\n",
    "        old = self.sumList[self.tail]\n",
    "        self.sumList[self.tail] = experience.priority\n",
    "        if old == 0:\n",
    "            change = experience.priority\n",
    "            self.currentSize += 1\n",
    "        else:\n",
    "            change = experience.priority - old\n",
    "        self.propagate(self.tail, change)\n",
    "        self.tail += 1\n",
    "\n",
    "    def propagate(self, index, change):\n",
    "        \"\"\"Updates sum tree to reflect change in priority of leaf.\"\"\"\n",
    "        parent = index // 2\n",
    "        if parent == 0:\n",
    "            return\n",
    "        self.sumList[parent] += change\n",
    "        self.propagate(parent, change)\n",
    "\n",
    "    def get_sum(self):\n",
    "        \"\"\"Return total sum of priorities.\"\"\"\n",
    "        return self.sumList[1]\n",
    "\n",
    "    def retrieve(self, start_index, num):\n",
    "        \"\"\"Return experience at index in which walking the array and summing the probabilities equals num.\"\"\"\n",
    "        # Return experience if we reach leaf node\n",
    "        if self.left(start_index) > len(self.sumList) - 1:\n",
    "            return self.experiences[start_index], start_index\n",
    "        # If left sum is greater than num, we look in left subtree\n",
    "        if self.sumList[self.left(start_index)] >= num:\n",
    "            return self.retrieve(self.left(start_index), num)\n",
    "        # If left sum is not greater than num, we subtract the left sum and look in right subtree\n",
    "        return self.retrieve(self.right(start_index), num - self.sumList[self.left(start_index)])\n",
    "\n",
    "    def update(self, index, experience):\n",
    "        \"\"\"Updates experience with new priority.\"\"\"\n",
    "        self.experiences[index] = experience\n",
    "        old_e_priority = self.sumList[index]\n",
    "        self.sumList[index] = experience.priority\n",
    "        change = experience.priority - old_e_priority\n",
    "        self.propagate(index, change)\n",
    "\n",
    "    def left(self, index):\n",
    "        return index * 2\n",
    "\n",
    "    def right(self, index):\n",
    "        return index * 2 + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.experiences[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.currentSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(env, brain_name, agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    print_solved_flag=False            # warn when the the average reward reaches 13.0\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #state = env.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0 and print_solved_flag==False:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            print_solved_flag=True\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            #break\n",
    "    \n",
    "    print('\\nTraining finished! Saving weights...')\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is assumed that the environment file is in the same path that this notebook. By default, the Mac environment. If that is not the case, please change the first line according to the right path and the file that matches your OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.05\n",
      "Episode 200\tAverage Score: 2.64\n",
      "Episode 300\tAverage Score: 6.39\n",
      "Episode 400\tAverage Score: 8.26\n",
      "Episode 500\tAverage Score: 12.10\n",
      "Episode 541\tAverage Score: 13.07\n",
      "Environment solved in 541 episodes!\tAverage Score: 13.07\n",
      "Episode 600\tAverage Score: 14.39\n",
      "Episode 700\tAverage Score: 15.89\n",
      "Episode 800\tAverage Score: 16.48\n",
      "Episode 900\tAverage Score: 15.94\n",
      "Episode 1000\tAverage Score: 16.50\n",
      "Episode 1100\tAverage Score: 16.53\n",
      "Episode 1200\tAverage Score: 16.36\n",
      "Episode 1300\tAverage Score: 16.94\n",
      "Episode 1400\tAverage Score: 16.45\n",
      "Episode 1500\tAverage Score: 17.26\n",
      "Episode 1600\tAverage Score: 16.19\n",
      "Episode 1700\tAverage Score: 16.77\n",
      "Episode 1800\tAverage Score: 16.53\n",
      "Episode 1900\tAverage Score: 16.55\n",
      "Episode 2000\tAverage Score: 15.77\n",
      "\n",
      "Training finished! Saving weights...\n",
      "To finish the program, manually close the plot window.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Banana.app\", worker_id=1)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# instantiate agent\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "# start training\n",
    "scores = dqn(env, brain_name, agent)\n",
    "\n",
    "# close environment\n",
    "#env.close()\n",
    "\n",
    "# Write scores in file for later plot edition:\n",
    "#np.save(\"scores\", scores)  \n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to watch the agent act according to he learnt.\n",
    "\n",
    "If you get the following error trying to run the next cell `OSError: handle is closed`, shutdown and reinitiate this notebook and run only this last cell again. For more info about this bug go to https://github.com/Unity-Technologies/ml-agents/issues/1167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Banana.app\", worker_id=2)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Instantiate agent:\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "#agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    #state = env.reset()\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        #state, reward, done, _ = env.step(action)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
